{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating offset for ECCO reuse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this code you should be able to calculate the correct start and end values for the reuse data so that the Octavo Reader works correctly. \n",
    "\n",
    "I am no expert in Python or data structures and algorithms, so things are not very optimised, but this seems to run in reasonable time so did not spend too much time optimizing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the libraries we need for this.\n",
    "If some of the libraries are not importing you probably have to install them first. \n",
    "Google might answer better how to do that than I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the path to the folder where your offset data is. You should not have other files or folders there.\n",
    "\n",
    "Fast way to do this is to locate one of the files from that folder and right click that. Then choose properties and copy the path that is inside there. \n",
    "\n",
    "At least with Windows the path will contain backslashes which is an escape character so you need to add another backslash before the backslashes for them to work. Slashes instead of double backslahs might work too. There is an example below.\n",
    "\n",
    "Characters like Å, Ä, Ö might cause problems with paths so you might want to avoid them.\n",
    "\n",
    "Relative paths should also be cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this path with your own. \n",
    "offsetPath = \"C:\\\\Users\\\\mikko\\\\ECCO_data\\\\offset_data\\\\\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get a list of all the offset files in that folder.\n",
    "You can print the list to make sure that all the files are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayOfOffsetFiles = os.listdir(offsetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arrayOfOffsetFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make a new dictionary that contains all the JSON objects in the files as dictionaries. This will make it easy to search the right ids later.\n",
    "\n",
    "I have commented out a piece of code where you can check if some id is actually in the data.\n",
    "If a id exist, console will print \"true\" and the name of the file it is in.\n",
    "\n",
    "When I was running this, some ids were missing and some were in multiple files, so if the offsets are not working after\n",
    "running this, it might be worth it to check that the id is actually in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsetObject = dict()\n",
    "\n",
    "for name in arrayOfOffsetFiles:\n",
    "    file = open(offsetPath + name, encoding=\"utf8\")\n",
    "    data = json.load(file)\n",
    "\n",
    "    #Replace the id with your own\n",
    "    #if data.get(\"1702800101\"):\n",
    "    #      print(name)\n",
    "    #      print(\"true\")\n",
    "\n",
    "    offsetObject = {**offsetObject, **data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to locate the path of the reuse data folder.\n",
    "Same procedure as with the offset data. \n",
    "Again check that there ar eno other files and folders there or it will try to run this on those too, which will probably lead to errors or unnecessary work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this path with your own. \n",
    "dataPath = \"C:\\\\Users\\\\mikko\\\\ECCO_data\\\\spectator1720\\\\\"\n",
    "dataFiles = arrayOfDataFiles = os.listdir(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is function a function for actually calculating the new offset.\n",
    "It takes as properties the following things: current row we are changing, column name where the id is located and column where the start/end point of the reuse is located.\n",
    "\n",
    "You probably don't need to change anything here unless you want to optimize my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcOffset(row, id_column, char_column):\n",
    "    \n",
    "    # The value of start/end currently\n",
    "    base = int(row[char_column])\n",
    "    \n",
    "    # The cumulative offset we will add to base\n",
    "    biggestSmaller = None\n",
    "    \n",
    "    # The offset dictionary that matches the id of the current row.\n",
    "    # Here I have added some zeros as padding to the left side of the id since the offset data and reuse data had the ids in\n",
    "    # different form\n",
    "    offset = offsetObject.get(str(row[id_column]).zfill(10))\n",
    "        \n",
    "    #If the offset is not found we can just return the initial value\n",
    "    if not offset:\n",
    "        return pd.Series([base, None])\n",
    "    \n",
    "    # Determining the new offset works here so that we are looking for the biggest offset point from the offset data that\n",
    "    # is smaller than the current position. After finding that we can just add the cumulative offset to the current position.\n",
    "    # Currently we are checking all the items in dictionary which is not very optimized. \n",
    "    for key, value in offset.items():\n",
    "        if int(key) < base and (not biggestSmaller or int(key) > int(biggestSmaller[0])):\n",
    "            biggestSmaller = (key, value)\n",
    "        \n",
    "    if not biggestSmaller:\n",
    "        return pd.Series([base, None])\n",
    "    \n",
    "    return pd.Series([base + biggestSmaller[1].get(\"offset\"), biggestSmaller[1].get(\"header\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to run this function to all rows of all data files.\n",
    "\n",
    "If you have different column names, you might need to change the parameters, but you probably don't need to.\n",
    "\n",
    "Choose a folder where you want to save the new files. This should probably be other folder than the one where the current data is. Also make sure that the folder actually exists.\n",
    "\n",
    "The new start and end points are stored in new columns called \"offsetPrimaryStart\" and such, but those can also be changed if need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From optimization point of view you should probably run just a single lambda to make it faster, but I did not bother.\n",
    "# In another words, this might take a moment.\n",
    "\n",
    "for file in dataFiles:\n",
    "\n",
    "    currentFile = pd.read_csv(dataPath + file)\n",
    "    \n",
    "    \n",
    "    # Changing the name in the brackets allows you to change the column where the new value is created\n",
    "    currentFile[['offsetPrimaryStart', 'primaryStartHeader']] = currentFile.apply(\n",
    "    #Changing these attributes allows you to change from which columns the values are read\n",
    "     lambda row: calcOffset(row, \"id_primary\", \"text_start_primary\"), axis=1\n",
    "    )\n",
    "    currentFile[['offsetPrimaryEnd', 'primaryEndHeader']] = currentFile.apply(\n",
    "     lambda row: calcOffset(row, \"id_primary\", \"text_end_primary\"), axis=1\n",
    "    )\n",
    "    currentFile[['offsetSecondaryStart', 'secondaryStartHeader']] = currentFile.apply(\n",
    "     lambda row: calcOffset(row, \"id_secondary\", \"text_start_secondary\"), axis=1\n",
    "    )\n",
    "    currentFile[['offsetSecondaryEnd', 'secondaryEndHeader']] = currentFile.apply(\n",
    "     lambda row: calcOffset(row, \"id_secondary\", \"text_end_secondary\"), axis=1\n",
    "    )\n",
    "    \n",
    "    # Here you can change where the new files are saved to. Keep the file in the end so you save each file to different file.\n",
    "    # For example, in my case this would result in file names such as:\n",
    "    # \"C:\\Users\\mikko\\ECCO_data\\fixedOffset\\spectator1720\\fixedOffset_spectator1.csv\"\n",
    "    currentFile.to_csv(\"C:\\\\Users\\\\mikko\\\\ECCO_data\\\\fixedOffset\\\\spectator1720\\\\fixedOffset_\" + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
